{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from project import *\n",
    "from evaluation.metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(DATA_DIR, \"market_bot\")\n",
    "DATASET_DIR = \"/home/itsnamgyu/data/market1501\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = np.load(os.path.join(data_path, \"qf.npy\"))\n",
    "gf = np.load(os.path.join(data_path, \"gf.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = torch.Tensor(qf)\n",
    "gf = torch.Tensor(gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_distances(qf, gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcamids = np.load(os.path.join(data_path, \"qcamids.npy\"))\n",
    "gcamids = np.load(os.path.join(data_path, \"gcamids.npy\"))\n",
    "qpids = np.load(os.path.join(data_path, \"qpids.npy\"))\n",
    "gpids = np.load(os.path.join(data_path, \"gpids.npy\"))\n",
    "qpaths = np.load(os.path.join(data_path, \"qpaths.npy\"))\n",
    "gpaths = np.load(os.path.join(data_path, \"gpaths.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcamids = torch.Tensor(qcamids)\n",
    "gcamids = torch.Tensor(gcamids)\n",
    "qpids = torch.Tensor(qpids)\n",
    "gpids = torch.Tensor(gpids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = distances[:, gpids != -1]  # filter junk galleries (missing id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_overlap = (qpids.reshape(-1, 1) == gpids)\n",
    "camera_overlap = (qcamids.reshape(-1, 1) == gcamids)\n",
    "junk = camera_overlap & person_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = distances.argsort(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3368, 15913])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_gp = gpids[ranks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = (ranked_gp == qpids.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1174., 1067., 1067.,  ...,  625.,    0.,    0.])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_gp[728]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3368])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_ranks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ranks = (matches * torch.arange(0, g)).float().mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 137,  184,  278,  359,  493,  580,  593,  630,  689,  728,  794,  832,\n",
       "          938, 1021, 1205, 1334, 1341, 1393, 1559, 1811, 1897, 2196, 2416, 2479,\n",
       "         2560, 2627, 2658, 2749, 2967, 3252]),)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(mean_ranks > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   0,    1,    2,    3,    4,    5,    6,    8,   11,   12,   34,   35,\n",
       "           36,   39,   46,   68,   73,  178,  185,  204,  461, 1090, 1553, 1645,\n",
       "         2425, 5117, 5357]),)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(matches[137])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gallery_image(index):\n",
    "    return Image.open(os.path.join(DATASET_DIR, gpaths[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_query_image(index):\n",
    "    return Image.open(os.path.join(DATASET_DIR, qpaths[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.1203, 6.4899, 6.5363,  ..., 6.2125, 6.7233, 6.1606],\n",
       "        [4.9783, 6.6089, 6.8476,  ..., 3.4175, 6.4988, 6.2843],\n",
       "        [6.4559, 6.1467, 7.5682,  ..., 6.9706, 6.4142, 6.4559],\n",
       "        ...,\n",
       "        [7.6159, 5.5724, 5.7081,  ..., 9.4089, 6.1018, 6.1894],\n",
       "        [7.1355, 5.4113, 5.2549,  ..., 8.5613, 5.1399, 4.6796],\n",
       "        [6.6259, 5.8003, 5.3014,  ..., 8.9103, 5.6569, 6.3106]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_index = np.argwhere(gl==ql)\n",
    "camera_index = np.argwhere(gc==qc)\n",
    "\n",
    "good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n",
    "junk_index1 = np.argwhere(gl==-1)\n",
    "junk_index2 = np.intersect1d(query_index, camera_index)\n",
    "junk_index = np.append(junk_index2, junk_index1) #.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
